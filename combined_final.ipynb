{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset \n",
    "import torch.nn.utils as utils\n",
    "import time\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import Levenshtein as lev\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "LETTER_LIST = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ','<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(LETTER_LIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  6 08:16:11 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   30C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(letter_list):\n",
    "    letter2index = {}\n",
    "    index2letter = {}\n",
    "    for i in range(0,len(letter_list)):\n",
    "        letter2index[letter_list[i]]=i\n",
    "        index2letter[i]=letter_list[i]\n",
    "    \n",
    "    return letter2index, index2letter\n",
    "\n",
    "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number  Start time in milliseconds  End time in milliseconds  \\\n",
      "0       1                        1650                     10800   \n",
      "1       2                       53940                     58090   \n",
      "2       3                       58700                     61440   \n",
      "3       4                       62060                     66540   \n",
      "4       5                       66540                     69550   \n",
      "\n",
      "                                                Text  \\\n",
      "0                                   TOHO CORPORATION   \n",
      "1  Some mornings, I wake up crying without knowin...   \n",
      "2      That's when everything happens now and again.   \n",
      "3  Whatever that dream was I had, I can never rem...   \n",
      "4                                  - But... - But...   \n",
      "\n",
      "                                            idx_text  \n",
      "0  [20, 15, 8, 15, 41, 3, 15, 18, 16, 15, 18, 1, ...  \n",
      "1  [19, 15, 13, 5, 41, 13, 15, 18, 14, 9, 14, 7, ...  \n",
      "2  [20, 8, 1, 20, 19, 41, 23, 8, 5, 14, 41, 5, 22...  \n",
      "3  [23, 8, 1, 20, 5, 22, 5, 18, 41, 20, 8, 1, 20,...  \n",
      "4  [41, 2, 21, 20, 37, 37, 37, 41, 41, 2, 21, 20,...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_table(\"knnw_en_sub.csv\", sep = \";\", header=0)\n",
    "idx_text = []\n",
    "for i in range(df.shape[0]):\n",
    "    text = df.iloc[i][3].lower()\n",
    "    split_text = text.split()\n",
    "    \n",
    "    l = []\n",
    "    for word in split_text:\n",
    "        if word[0] == \"[\" or word[-1] == \":\": #remove speaker name and noise\n",
    "            continue\n",
    "        for c in word:\n",
    "            if c == 'é':\n",
    "                c = 'e'\n",
    "            if c == \"“\" or c == \"”\":\n",
    "                c = '\"'\n",
    "            if c in letter2index:\n",
    "                l.append(letter2index[c])\n",
    "        l.append(letter2index[' '])#Space between the words use letter2index\n",
    "    l=l[:-1]\n",
    "    l.append(letter2index['<eos>'])#End of the sentence\n",
    "    idx_text.append(l)\n",
    "df['idx_text'] = idx_text\n",
    "print(df.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "{'Mitsuha': 0, 'Taki': 1, 'Band': 2, 'Yotsuha': 3, 'Grandma': 4, 'Yatsuha': 5, 'Radio': 6, 'None': 7, 'TV': 8, 'Saya': 9, 'Teshi': 10, 'Toshiki': 11, 'BystanderA': 12, 'BystanderB': 13, 'StudentA': 14, 'BystanderC': 15, 'BystanderD': 16, 'StudentB': 17, 'StudentC': 18, 'Sara': 19, 'Teacher': 20, 'StudentD': 21, 'Class': 22, 'Foreman': 23, 'BackCharA': 24, 'BackCharB': 25, 'TeshiMom': 26, 'BystanderE': 27, 'BystanderF': 28, 'TakiDad': 29, 'Crowd': 30, 'Tsukasa': 31, 'Shinta': 32, 'Kitchen': 33, 'Customers': 34, 'Okudera': 35, 'Staff': 36, 'Phone': 37, 'Takagi': 38, 'Waitress': 39, 'Chef': 40, 'Futaba': 41, 'MitsuhaMom': 42, 'Gradnma': 43, 'Shopkeeper': 44, 'Mistuha': 45, 'Kids': 46, 'Conductor': 47, 'TownHall': 48, 'Firefighters': 49, 'TeshiDad': 50, 'TrainAnnouncer': 51}\n",
      "1393\n",
      "(1393, 52)\n"
     ]
    }
   ],
   "source": [
    "file = open(\"speakers.srt\", \"r\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "count = 2\n",
    "speakers = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.startswith(str(count)):\n",
    "        count = count + 1\n",
    "        speakers.append(line)\n",
    "\n",
    "name_list = []\n",
    "for i in range(2, len(speakers)+2):\n",
    "    new_str = speakers[i-2].replace(str(i), '')\n",
    "    new_str = new_str.replace('\\n', '')\n",
    "    new_str = new_str.replace(' ', '')\n",
    "    new_str = new_str.replace(',', '')\n",
    "    speakers[i-2] = new_str\n",
    "    if '+' in new_str:\n",
    "        new_str = new_str.split('+')\n",
    "        for name in new_str:\n",
    "            if name not in name_list:\n",
    "                name_list.append(name)\n",
    "    else:\n",
    "        if new_str not in name_list:\n",
    "            name_list.append(new_str)\n",
    "print(len(name_list))\n",
    "name_dict = {}\n",
    "for i in range(len(name_list)):\n",
    "    name_dict[name_list[i]] = i\n",
    "print(name_dict)\n",
    "\n",
    "speakers = ['None'] + speakers\n",
    "print(len(speakers))\n",
    "\n",
    "speaker_label = []\n",
    "for i in range(len(speakers)):\n",
    "    l = np.zeros(52)\n",
    "    if '+' in speakers[i]:\n",
    "        new_str = speakers[i].split('+')\n",
    "        for n in new_str:\n",
    "            l[name_dict[n]] = 1\n",
    "    else:\n",
    "        if speakers[i] != 'None':\n",
    "            l[name_dict[speakers[i]]] = 1\n",
    "    \n",
    "    speaker_label.append(l)\n",
    "speaker_label = np.array(speaker_label)\n",
    "print(speaker_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129, 629570)\n",
      "1393\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "total_frames = 629570\n",
    "# total_frames = 199864\n",
    "total_duration=6396010\n",
    "duration_per_frame = total_duration / total_frames\n",
    "audio = np.load(\"log_spectrogram.npy\")\n",
    "# audio = np.load(\"knnw16000.npy\")\n",
    "print(audio.shape)\n",
    "\n",
    "def get_index(time, start_flag):\n",
    "    if start_flag == True:\n",
    "        return np.floor(time/duration_per_frame)\n",
    "        \n",
    "    else:\n",
    "        return np.ceil(time/duration_per_frame)\n",
    "        \n",
    "def get_range(start_time, end_time):\n",
    "        \n",
    "    start_index = get_index(start_time, start_flag=True)\n",
    "    stop_index  = get_index(end_time, start_flag=False)\n",
    "        \n",
    "    return range(int(start_index), int(stop_index))\n",
    "\n",
    "temp = []\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    start_time = df.iloc[i][1]\n",
    "    stop_time = df.iloc[i][2]\n",
    "    \n",
    "    audio_range = get_range(start_time, stop_time)\n",
    "    audio_item = torch.as_tensor(audio[:,audio_range]).float().permute(1,0)\n",
    "    temp.append([audio_item, df.iloc[i][4]])\n",
    "print(len(temp))\n",
    "\n",
    "temp_speaker = []\n",
    "for i in range(df.shape[0]):\n",
    "    \n",
    "    start_time = df.iloc[i][1]\n",
    "    stop_time = df.iloc[i][2]\n",
    "    \n",
    "    audio_range = get_range(start_time, stop_time)\n",
    "    audio_item = torch.as_tensor(audio[:,audio_range]).float().permute(1,0)\n",
    "    temp_speaker.append([audio_item, speaker_label[i]])\n",
    "print(len(temp_speaker))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1393\n",
      "1393\n",
      "1393\n",
      "1393\n",
      "1393\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(temp)\n",
    "print(len(temp))\n",
    "audio_array = []\n",
    "idx_text = []\n",
    "for i in range(len(temp)):\n",
    "    audio_array.append(temp[i][0])\n",
    "    idx_text.append(temp[i][1])\n",
    "print(len(audio_array))\n",
    "print(len(idx_text))\n",
    "\n",
    "random.shuffle(temp_speaker)\n",
    "print(len(temp_speaker))\n",
    "audio_speaker = []\n",
    "speaker_label = []\n",
    "for i in range(len(temp_speaker)):\n",
    "    audio_speaker.append(temp_speaker[i][0])\n",
    "    speaker_label.append(temp_speaker[i][1])\n",
    "print(len(audio_speaker))\n",
    "print(len(speaker_label))\n",
    "speaker_label = np.array(speaker_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech2TextDataset(Dataset):\n",
    "    '''\n",
    "    Dataset class for the speech to text data, this may need some tweaking in the\n",
    "    getitem method as your implementation in the collate function may be different from\n",
    "    ours. \n",
    "    '''\n",
    "    def __init__(self, speech, text=None, speaker=None, isTrain=True):\n",
    "        self.speech = speech\n",
    "        self.speaker = speaker\n",
    "        self.isTrain = isTrain\n",
    "        if (text is not None):\n",
    "            self.text = text\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if (self.isTrain == True):\n",
    "            return self.speech[index], torch.tensor(self.text[index]), torch.tensor(self.speaker[index].astype(np.long))\n",
    "        else:\n",
    "            return self.speech[index]\n",
    "\n",
    "def collate_train(batch_data):\n",
    "    #This batch will contain the speech,speech len and the text,text len\n",
    "    ### Return the padded speech and text data, and the length of utterance and transcript ###\n",
    "    X = []\n",
    "    Xlen = []\n",
    "    Y = []\n",
    "    Ylen = []\n",
    "    Z = []\n",
    "    Zlen = []\n",
    "    #print(\"Length of batch data :\",len(batch_data))\n",
    "    for i in range(len(batch_data)):\n",
    "        X.append(batch_data[i][0])\n",
    "        Xlen.append(len(batch_data[i][0]))\n",
    "        Y.append(batch_data[i][1])\n",
    "        Ylen.append(len(batch_data[i][1]))\n",
    "        Z.append(batch_data[i][2])\n",
    "        Zlen.append(len(batch_data[i][2]))\n",
    "        \n",
    "    Xpad=pad_sequence(X, batch_first=True)#(B, T, C) \n",
    "    Ypad=pad_sequence(Y, batch_first=True)#(B, T, C) \n",
    "    Zpad=pad_sequence(Z, batch_first=True)\n",
    "    \n",
    "    return Xpad,torch.tensor(Xlen),Ypad,torch.tensor(Ylen), Zpad,torch.tensor(Zlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 11785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115\n",
      "278\n"
     ]
    }
   ],
   "source": [
    "split = int(validation_split * len(audio_array))\n",
    "train_dataset = Speech2TextDataset(audio_array[split:], idx_text[split:], speaker_label[split:])\n",
    "valid_dataset = Speech2TextDataset(audio_array[:split], idx_text[:split], speaker_label[:split])\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_train,num_workers=8,pin_memory=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_train,num_workers=8,pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.2):\n",
    "        # x': (B, L, C)\n",
    "#         if dropout == 0 or not self.training:\n",
    "#             return x\n",
    "        mask = x.data.new(x.size(0), 1, x.size(2))\n",
    "        mask = mask.bernoulli_(1 - dropout)\n",
    "        mask = Variable(mask, requires_grad=False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        \n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Attention is calculated using key, value and query from Encoder and decoder.\n",
    "    Below are the set of operations you need to perform for computing attention:\n",
    "        energy = bmm(key, query)\n",
    "        attention = softmax(energy)\n",
    "        context = bmm(attention, value)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, query, key, value, lens):\n",
    "        '''\n",
    "        :param query :(batch_size, hidden_size) Query is the output of LSTMCell from Decoder\n",
    "        :param keys: (batch_size, max_len, encoder_size) Key Projection from Encoder\n",
    "        :param values: (batch_size, max_len, encoder_size) Value Projection from Encoder\n",
    "        :return context: (batch_size, encoder_size) Attended Context\n",
    "        :return attention_mask: (batch_size, max_len) Attention mask that can be plotted \n",
    "        \n",
    "        '''\n",
    "        attention = torch.bmm(key, query.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        mask = torch.arange(key.size(1)).unsqueeze(0) >= lens.unsqueeze(1)\n",
    "        mask=mask.to(device)\n",
    "        attention.masked_fill_(mask, -1e9)\n",
    "        attention = nn.functional.softmax(attention, dim=1)\n",
    "        context = torch.bmm(attention.unsqueeze(1), value).squeeze(1)\n",
    "\n",
    "        return context, attention\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pBLSTM(nn.Module):\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    The length of utterance (speech input) can be hundereds to thousands of frames long.\n",
    "    The Paper reports that a direct LSTM implementation as Encoder resulted in slow convergence,\n",
    "    and inferior results even after extensive training.\n",
    "    The major reason is inability of AttendAndSpell operation to extract relevant information\n",
    "    from a large number of input steps.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, dropout_rate=0.1):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        self.blstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=2,dropout=dropout_rate, bidirectional=True,batch_first=True)\n",
    "        self.lockdrop = LockedDropout()\n",
    "        self.dropout=dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x :(N, T) input to the pBLSTM\n",
    "        :return output: (N, T, H) encoded sequence from pyramidal Bi-LSTM \n",
    "        '''\n",
    "        #First we unpack the sequence\n",
    "        x_pad,x_len=utils.rnn.pad_packed_sequence(x,batch_first=True)\n",
    "\n",
    "        x_pad = self.lockdrop(x_pad, self.dropout)\n",
    "        x_pad=x_pad[:,:(x_pad.shape[1]//2)*2,:]\n",
    "        \n",
    "        x_pad = x_pad.contiguous().view(x_pad.shape[0],int(x_pad.shape[1]//2),x_pad.shape[2]*2)        \n",
    "\n",
    "        #print(\"Shape of output after x_pad :\",x_pad.shape)\n",
    "        x_len=x_len//2\n",
    "\n",
    "        #Now we pack this back to send it off to the LSTM\n",
    "        x_pack = utils.rnn.pack_padded_sequence(x_pad, lengths=x_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        output, _ = self.blstm(x_pack)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Encoder takes the utterances as inputs and returns the key and value.\n",
    "    Key and value are nothing but simple projections of the output from pBLSTM network.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, value_size=256,key_size=256,dropout_rate=0.0,dropouti=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=256, num_layers=5, dropout=0.5, bidirectional=True,batch_first=True)\n",
    "\n",
    "        self.pBLSTM1=pBLSTM(hidden_dim*4, hidden_dim,dropout_rate=dropout_rate)\n",
    "        self.pBLSTM2=pBLSTM(hidden_dim*4, hidden_dim,dropout_rate=dropout_rate)\n",
    "        self.pBLSTM3=pBLSTM(hidden_dim*4, hidden_dim,dropout_rate=dropout_rate)\n",
    "        \n",
    "        self.key_network = nn.Linear(hidden_dim*2, value_size)\n",
    "        self.value_network = nn.Linear(hidden_dim*2, key_size)\n",
    "        \n",
    "        self.dropout=dropouti\n",
    "        self.lockdrop = LockedDropout()\n",
    "\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        rnn_inp = utils.rnn.pack_padded_sequence(x, lengths=lens, batch_first=True, enforce_sorted=False)\n",
    "        dia_input, _ = self.lstm(rnn_inp)\n",
    "        \n",
    "        outputs=self.pBLSTM1(dia_input)\n",
    "        outputs=self.pBLSTM2(outputs)\n",
    "        outputs=self.pBLSTM3(outputs)\n",
    "        \n",
    "        linear_input,encoder_lens = utils.rnn.pad_packed_sequence(outputs,batch_first=True)\n",
    "        \n",
    "        linear_input = self.lockdrop(linear_input, self.dropout)\n",
    "        keys = self.key_network(linear_input)\n",
    "        value = self.value_network(linear_input)\n",
    "\n",
    "        return keys, value,encoder_lens, dia_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step, \n",
    "    thus we use LSTMCell instead of LSLTM here.\n",
    "    The output from the second LSTMCell can be used as query here for attention module.\n",
    "    In place of value that we get from the attention, this can be replace by context we get from the attention.\n",
    "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
    "    '''\n",
    "    def __init__(self, vocab_size, hidden_dim, value_size=256, key_size=256):\n",
    "        super(Decoder, self).__init__()\n",
    "        hidden_dim=key_size + value_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=letter2index['<eos>'])\n",
    "        self.lstm1 = nn.LSTMCell(input_size=768, hidden_size=256)\n",
    "        self.lstm2 = nn.LSTMCell(input_size=256, hidden_size=key_size)\n",
    "        \n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.character_prob = nn.Linear(key_size + value_size, vocab_size)\n",
    "        self.value_size=value_size \n",
    "        self.embedding.weight=self.character_prob.weight\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.character_prob.bias.data.zero_()\n",
    "        self.character_prob.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "\n",
    "    def forward(self, epoch,batch_idx,encoder_lens,teacherForcingRate,key, values, text=None, isTrain=True):\n",
    "        '''\n",
    "        :param key :( N,T, key_size) Output of the Encoder Key projection layer\n",
    "        :param values: (N, T,value_size) Output of the Encoder Value projection layer\n",
    "        :param text: (N, text_len) Batch input of text with text_length\n",
    "        :param isTrain: Train or eval mode\n",
    "        :return predictions: Returns the character perdiction probability \n",
    "        '''\n",
    "        batch_size = key.shape[0]\n",
    "\n",
    "        if (isTrain == True):\n",
    "            max_len =  text.shape[1]\n",
    "            embeddings = self.embedding(text)\n",
    "        else:\n",
    "            max_len = 600\n",
    "            \n",
    "        total_attention=[]\n",
    "        predictions = []\n",
    "        hidden_states = [None, None]\n",
    "        prediction = torch.zeros(batch_size,1).to(device)\n",
    "        context = values[:, 0, :].reshape(values.size(0),values.size(2))\n",
    "        teacherForcingRate=teacherForcingRate \n",
    "        \n",
    "        for i in range(max_len):\n",
    "            \n",
    "            if (isTrain):\n",
    "                #Teacher forcing\n",
    "                tf = np.random.random()\n",
    "                if tf > teacherForcingRate :\n",
    "                    if i==0:\n",
    "                        char_embed=self.embedding(torch.zeros(batch_size,dtype=torch.long).fill_(letter2index['<sos>']).to(device))\n",
    "                    else:\n",
    "                        char_embed = embeddings[:,i-1,:].to(device)#Dimension is of B,1,hidden_dim\n",
    "                else:\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1)).to(device)\n",
    "                    \n",
    "            else:\n",
    "                if i==0:\n",
    "                    char_embed=self.embedding(torch.zeros(batch_size,dtype=torch.long).fill_(letter2index['<sos>']).to(device))\n",
    "                else:\n",
    "                    char_embed = self.embedding(prediction.argmax(dim=-1)).to(device)\n",
    "\n",
    "                \n",
    "            inp = torch.cat([char_embed, context], dim=1)#Char Embed dim:B,H_ and Context:B,hidden_dim\n",
    "            hidden_states[0] = self.lstm1(inp, hidden_states[0])\n",
    "\n",
    "            inp_2 = hidden_states[0][0]\n",
    "            hidden_states[1] = self.lstm2(inp_2, hidden_states[1])\n",
    "\n",
    "            ### Compute attention from the output of the second LSTM Cell ###\n",
    "            output = hidden_states[1][0]\n",
    "                        \n",
    "            context,masked_attention=self.attention(output,key,values, encoder_lens)\n",
    "            total_attention.append(masked_attention.detach().cpu())\n",
    "\n",
    "            prediction = self.character_prob(torch.cat([output, context], dim=1))\n",
    "                                                                                 \n",
    "            predictions.append(prediction.unsqueeze(1))\n",
    "            \n",
    "        if batch_idx+1==9:\n",
    "            \n",
    "            all_attentions = torch.stack(total_attention, dim=1)\n",
    "            \n",
    "            plt.clf()\n",
    "            \n",
    "            sns.heatmap(all_attentions[0,:,:], cmap='GnBu')\n",
    "            plt.show()\n",
    "            print(\"all_attentions shape :\",all_attentions.shape)\n",
    "            \n",
    "\n",
    "        return torch.cat(predictions, dim=1)#B,T,V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    '''\n",
    "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
    "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
    "    '''\n",
    "    def __init__(self, input_dim, vocab_size, hidden_dim, value_size=128, key_size=128, isAttended=True):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.dropout_rate=0.1\n",
    "        self.dropouti=0.1\n",
    "        self.encoder = Encoder(input_dim,256,256,256,self.dropout_rate,self.dropouti)\n",
    "        self.decoder = Decoder(vocab_size, hidden_dim,256,256)\n",
    "        self.fc1 = nn.Linear(512, 52)\n",
    "        initrange = 0.1\n",
    "        self.fc1.bias.data.zero_()\n",
    "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, epoch,batch_idx,teacherForcingRate,speech_input, speech_len, text_input=None, isTrain=True):\n",
    "        key, value, encoder_lens, dia_input = self.encoder(speech_input, speech_len)\n",
    "        dia_input,_ = utils.rnn.pad_packed_sequence(dia_input,batch_first=True)\n",
    "        dia_output = self.fc1(dia_input)\n",
    "        if (isTrain == True):\n",
    "            predictions = self.decoder(epoch,batch_idx,encoder_lens,teacherForcingRate,key, value, text_input,isTrain=True)\n",
    "        else:\n",
    "            predictions = self.decoder(epoch,batch_idx,encoder_lens,None,key, value, text=None, isTrain=False)\n",
    "        return predictions, dia_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(pred):\n",
    "    sentence_full=[]\n",
    "    for sentence in pred:#Taking every sentence\n",
    "        sent=''\n",
    "        for char in sentence:\n",
    "            if char==33:\n",
    "                break\n",
    "            else:\n",
    "                sent+=index2letter[char]\n",
    "        sentence_full.append(sent)\n",
    "    return sentence_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For computing the distance\n",
    "def LevScore(preds, targets):\n",
    "    res = 0.0\n",
    "    for i in range(len(preds)):\n",
    "        dist = lev.distance(preds[i], targets[i])\n",
    "        res += dist\n",
    "        \n",
    "    return res#Average Levenstein Distance over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "#now we should build the model\n",
    "print(len(LETTER_LIST))\n",
    "model = Seq2Seq(input_dim=129, vocab_size=len(LETTER_LIST), hidden_dim=256)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "criterion_dia = nn.CrossEntropyLoss(reduction='none')\n",
    "nepochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(129, 256, num_layers=5, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (pBLSTM1): pBLSTM(\n",
       "      (blstm): LSTM(1024, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "      (lockdrop): LockedDropout()\n",
       "    )\n",
       "    (pBLSTM2): pBLSTM(\n",
       "      (blstm): LSTM(1024, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "      (lockdrop): LockedDropout()\n",
       "    )\n",
       "    (pBLSTM3): pBLSTM(\n",
       "      (blstm): LSTM(1024, 256, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "      (lockdrop): LockedDropout()\n",
       "    )\n",
       "    (key_network): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (value_network): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (lockdrop): LockedDropout()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(43, 512, padding_idx=42)\n",
       "    (lstm1): LSTMCell(768, 256)\n",
       "    (lstm2): LSTMCell(256, 256)\n",
       "    (attention): Attention()\n",
       "    (character_prob): Linear(in_features=512, out_features=43, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=52, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch, teacherForcingRate, criterion_dia):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    train_loss=0\n",
    "    dia_loss = 0\n",
    "    \n",
    "    \n",
    "    for batch_idx,(inp,inp_len,target,target_len,speaker,speaker_len) in enumerate(train_loader):\n",
    "        start = time.time()\n",
    "        \n",
    "        # Set the inputs to the device.\n",
    "        inp,target,speaker = inp.to(device),target.to(device), speaker.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output, dia_output = model(epoch,batch_idx,teacherForcingRate,inp, inp_len, target,True)\n",
    "        \n",
    "        # Process Diarization Module\n",
    "        temp = torch.mean(dia_output, 1)\n",
    "        \n",
    "        speaker = torch.argmax(speaker, dim=1)\n",
    "        loss_dia = criterion_dia(temp,speaker)\n",
    "        \n",
    "        loss_dia = loss_dia.sum()\n",
    "\n",
    "        dia_loss += loss_dia.item()\n",
    "        \n",
    "        \n",
    "        # Process Recognition Module\n",
    "        mask = torch.arange(max(target_len)).unsqueeze(0) >= target_len.int().unsqueeze(1)#Switching Masks\n",
    "        mask=mask.to(device)\n",
    "\n",
    "        output.contiguous() \n",
    "        loss=criterion(output.view(-1,output.shape[2]),target.contiguous().view(-1))\n",
    "\n",
    "        loss.masked_fill_(mask.contiguous().view(-1),0)\n",
    "        \n",
    "        loss=loss.mean()\n",
    "        total_loss = loss + loss_dia\n",
    "        total_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 2)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "        del inp\n",
    "        del inp_len\n",
    "        del target\n",
    "        del target_len\n",
    "        del speaker\n",
    "        del speaker_len\n",
    "        stop = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('B: %d / %d, avg_loss: %.3f, avg_loss_dia: %.3f, Time Taken : %.3f, ' % (batch_idx+1, len(train_loader),train_loss/(batch_idx+1),dia_loss/(batch_idx+1),stop-start),end='\\n ')\n",
    "    \n",
    "\n",
    "def validation(model, valid_loader):\n",
    "    valid_loss=0\n",
    "    total_dist_score=0\n",
    "    model.eval()\n",
    "    seq_len=0\n",
    "    dist_score=0\n",
    "    correct_num = 0\n",
    "    dia_loss = 0.0\n",
    "    \n",
    "    for batch_idx,(inp,inp_len,target,target_len,speaker,speaker_len) in enumerate(valid_loader):\n",
    "        start = time.time()\n",
    "        inp,target,speaker = inp.to(device),target.to(device),speaker.to(device)\n",
    "        \n",
    "        output, dia_output = model(epoch,batch_idx,None,inp, inp_len, None, False)\n",
    "        \n",
    "        temp = torch.mean(dia_output, 1)\n",
    "        \n",
    "        speaker = torch.argmax(speaker, dim=1)\n",
    "        predicted = torch.argmax(temp, dim = 1)\n",
    "        print(predicted)\n",
    "        loss_dia = criterion(temp,speaker)\n",
    "        \n",
    "        for n in range(len(predicted)):\n",
    "            if predicted[n] == speaker[n]:\n",
    "                correct_num += 1\n",
    "        \n",
    "        loss_dia = loss_dia.sum()\n",
    "        dia_loss += loss_dia.item()\n",
    "        \n",
    "        pred = predictions(output.argmax(-1).detach().cpu().numpy())\n",
    "        target = predictions(target.detach().cpu().numpy())\n",
    "        \n",
    "\n",
    "        dist_score += LevScore(pred,target)\n",
    "        seq_len += len(pred)\n",
    "        \n",
    "        del inp\n",
    "        del inp_len\n",
    "        del target_len\n",
    "        del speaker\n",
    "        del speaker_len\n",
    "        stop = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"prediciton :\",pred[2])\n",
    "    print(\"Target : \",target[2])\n",
    "    print('Distance Score: %.3f ' % (dist_score/seq_len),end='\\n ')\n",
    "    print('Loss in Diarization:', dia_loss/seq_len, 'Correct prediction:', correct_num, 'se1_len:', seq_len)\n",
    "    return dist_score/seq_len, correct_num, dia_loss/seq_len, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mn = torch.load(\"epoch33\")\n",
    "# model.load_state_dict(mn['model_state'])\n",
    "# optimizer.load_state_dict(mn['opti_state'])\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_group in optimizer.param_groups:\n",
    "#         param_group['lr'] = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher Forcing : 0.15\n",
      "epoch: 0\n",
      "0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 1 / 9, avg_loss: 0.699, avg_loss_dia: 505.929, Time Taken : 2.276, \n",
      " "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAEECAYAAAAWBO4AAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkwUlEQVR4nO3de7xcVX338c93JiAJIDcFaRIELaJoHxAiqCiCCgZqpfVVLWhtvfJEQVFsq7bPq9ra2mj1QSjeAlisN0QFRYxAtEUfX14IlwiEgERqIUTDJQEvIOEkv+ePvc+cfSZ7ZvZczsw6k+87r/06M3v22nvNyTlr1ll7/dZPEYGZmQ1fbdQVMDPbXrkBNjMbETfAZmYj4gbYzGxE3ACbmY2IG2AzsxHpqwGWtFjSbZLWSnr3oCplZrY9UK/zgCXVgZ8CxwHrgJXAKRFxy+CqZ2Y2vub0UfYIYG1E3AEg6SLgJKBlA3zbgzcHwB477t7Yt0Mtq0JNU51xoW0Lq2RfDybPUvljp+QDamuh9OatmxuPH7vDrr1XzMym2am+W9+/9HOP/cdKv+oP/9ffD6aB6VI/QxDzgbsKz9fl+6aRdKqkayVd+6ULv9zH5czMxks/PeCyT4xtPm0iYhmwDOB3Wx7c9vW8SHEoZGtJ/zRia/kFgFqhKsWyjd5u4dwx2ZMu7svLlPW8o3C+idgCwOatjzb2XXvvrY3Hx80/sqR2ZjYy9fqoa9BWPw3wOmBh4fkCYH1/1TEzG6ABDV3OlH4a4JXAgZIOAO4GTgZe1a7AHid8HID7ly9p7Jsc+1XhG1U6LqLRf5I9Jv+6c2Gfe71mCRvXBjgiJiSdDlwJ1IFPR8TqgdXMzKxfSjvUoZ8eMBGxHFg+oLqYmQ1WbUx7wL24+xuvA6bfKIv8xlbp1LMZFNNu1qnSvsmbeb/b8khj35nf39B4vOzYJ81MZc2sN+M6BGFmlrza6O8dtTPUBnj+6d8F4BcfO7axb8faDsD0m3CTht0r7iivzi61qW+be71mCRvnIQhJPwd+DWwBJiJi0SAqZWY2EON8Ey53bETcV+XAfz5jb2B6+O7kNLQ624YidxqT7aRdD7rqeaaNAedft2ydaOzb8PA9jcf77VKcFm1mI+cxYDOzERnzHnAAV0kK4FN52PE0kk4FTgU49xMf5Q1vem1T2HEeYlwSGlwWkly2OE5x/Hja6m75/skwZpjqFZeFLDe/sebXJkORHy30gH/z6K9LSptZEurj3QAfFRHrJe0NrJB0a0R8r3hAp7UgzMxmTOI94L5qFxHr86/3AJeSLVFpZpYGqdo2Ij33gCXtDNQi4tf54+OBf2xXZo8Xnw3Axm+/rbGvTj5PL+2xcmBqLYii3Xc8eOj1MLOKxnga2j7Apfn46xzgCxFxxUBqZWY2CIkPQfSzGM8dwCHdlDn6LX8EwMMTv2vse0x9R2B6Roxk5SPYW/KbcQAbHr638Xj+zvsOu0Zm1o6noZmZjcgYL8jete99/BsAzHv5GcO87ODkH6ZzNPVtc6/XLGGJ94A7/t0v6dOS7pF0c2HfnpJWSLo9/7rHzFbTzKwHYzAL4kLgXOA/CvveDXwnIpZKenf+/F2dTrTp2617vpVDjGdgJnHVa28tBHRMLkl59L9N5YS7/szDB1sxM+tP4veWOtYuD6zY2LT7JOAz+ePPAH882GqlrbgesJklrKZq26iq12O5fSLiFwD5171bHVhMS3/BeRf2eDkzsx4kPgShKFlbYZuDpP2ByyPiGfnzByJi98LrmyKi4zjw3Be+PwA2rnjr1LlnQwSGmQ3dTvXd+m4c5r7pS5XGFx8+789G0hD12gPeIGlfgPzrPR2ONzMbvsSHIHqdhnYZ8JfA0vzr16sU2nDlm4HpN7Mme8BlPeGy9YDLXi++1u8awu3qMFnvR7ZMrWf8F8unAjEuOWn/nq9jZjNgtt+Ek/RF4IfAQZLWSXoDWcN7nKTbgePy52ZmaUl8DLhjDzgiTmnx0ou6vdg+J3wKgI1XntbY124MuNP4cNnrVff1oq4sqmaHPI8dwCUn7TyQc5vZ4JXlmkyJQ5HNbGzV6m6Ap+yeTZSYKGSUmJNnGPZsCDMbtMRXo+w5FPl9ku6WtCrfTpzZapqZdU9SpW1UqtwivBBYXLL/rIg4NN+WD7ZaZmb9S/weXKWbcN/LAzH6tulLr87OWUz1HpMJOKempsXUi+3rVnEaWlmZ0jOXXa/wvzM5dLJ566ONfSvuXt14/IoDntO2vmY2XKnfhOtnktzpkm7MhyhaRsE5FNnMRiX1IYheb8J9Ang/WUfy/cBHgNeXHVjMinzdfT+I1ZtWc8CuT5qqQH4TrpgRo/HtKHxjqgZilL1eVMs/c0qDM0pOUzxqh7yuxbLz55VlijOzFNQSvwvXUwMcERsmH0s6D7h8YDUyMxuQxAPhemuAJe07uRoa8CfAze2On/Sysx4A4Ob3Tn0q7TCZXaLwQZXqlLTI/zeLGTGesceBo6qOmXWQ+hhwxwY4D0U+BnicpHXAe4FjJB1K9hf6z4H/PXNVNDPrTeLtb8+hyBfMQF3MzAaqNsAWWNJi4GygDpwfEUubXt8N+BywH1nb+uGI+Pd25xxqJNzGm24DYIfasxv7Uv8ToagxNFKo8i47eC0Is1QNqn2RVAc+Rrb42DpgpaTLIuKWwmGnAbdExB9Jejxwm6TPR8TmklMCXgvCzMbYAGdBHAGsjYg7ACRdRJaardgAB7CrslZ/F7JUbhPNJ5pWv0HVrpKJCZiYQNDYZjsV/plZWqpGwhXjFfLt1KZTzQfuKjxfl+8rOhd4GrAeuAk4I6Kw+HmJKjfhFpJlRH4CsBVYFhFnS9oT+BKwP9mNuFdGxKZO5zMzGxZV7AEX4xVanaqsWNPzlwCrgBcCTwZWSPp/EfGrViet0gOeAN4ZEU8Dng2cJulgplLTHwh8J3/e3patsGXrePUax6k7bzZmBrgWxDpgYeH5ArKebtHrgEsisxb4b+Cp7U5aJS39LyLi+vzxr4E1ZF3v7To1vZmlb4ChyCuBAyUdIGlH4GSy1GxFd5InqpC0D3AQcEe7k3Y1BpwvyvNM4MdUTE1fHFuZWPfjbi5nZtaXWk2Vtk4iYgI4HbiSrBN6cUSslrRE0pL8sPcDz5V0E9mowLsi4r525608C0LSLsBXgbdHxK+qTu8ojq3MfckHe8+OaWbWpUFOc82X3V3etO+ThcfrgeO7OWelHrCkHcga389HxCX5bqemN7OkJZ6VvtIsCJFFvq2JiP9beKnr1PQbv5lHLBfecNuU8cWXVLKv+bUq5+nStLWL88fFtPTPOWtqGYyf/PWzer+QmQ1c1VkQo1JlCOIo4DXATZJW5fv+lqzhvThPU38n8IoZqaGZWY9SD7StshbE92ndx+wqNf2eJ34CgPuXL2nsq7VbL67sqr18Q/v4TyibKjdvztzG41V/vaj3k5vZjEp9qQOHIpvZ2BrLBdl7Vq8P9XJDUTZObWZJSL0HXCUt/UJJ/yVpjaTVks7I9zs1vZklbdZnRWYqFPl6SbsC10lakb92VkR8uPLVJrKFgcYiBDmX+ies2fZs1s+CyKPcJiPefi1pMhTZzCxpqfeP+glFhgqp6aeHIl/TX23NzLpQr9UqbaNS+crNochkqemfDBxK1kP+SFm5iFgWEYsiYtGchUem/5HUpSj8M7O0jMMYcGkoslPTm1nqZv0YcKtQ5F5T08P08N5xuiFnZmlJ/SZ5P6HIpzg1vZmlLPH2t69Q5OUl+8zMkjEOPWAzs1mpVncDbGY2Eqn3gKuEIu8k6RpJP8lDkf8h37+npBWSbs+/ls4DNjMblZpUaRtZ/Soc8wjwwog4hGzO72JJz6aXrMhmZkOU+jzgKlmRIyJ+kz/dId8CZ0U2s8SppkrbqFTNCVfPp6DdA6yIiB6zIjsU2cyGZ4Bp6WdEpQY4IrZExKHAAuAISc+oeoFpocgLjuixmmZm3RtUWvqZ0tUsiIh4QNLVwGLyrMgR8QtnRTazFI1DKPLjgUfzxncu8GLgg/SSFXn5m/OTTu2b7VmRjzp7dePxDe90fjizlCQ+C61SD3hf4DOS6mRDFhdHxOWSfoizIptZwlKfB1wlFPlGsjWAm/ffT49ZkRs9YTp8g2ZBVuTr33l47yc3sxk16xtgM7PZqu5QZDOz0Ui9B9xPKHL3WZHrdajXxyuLRBQ2M0tK6pFwVXrAk6HIv8kzY3xf0rfy17rLimxmNkSp94Cr3IQLoCwUuXtOS29mQ5T6POB+QpGhQlZkM7NRSX0Iop9Q5EpZkaetBXH3yvRnRndprMazzcZMrVartI2sft0cHBEPAFcDiyNiQ94wbwXOA0oXevBaEGY2KjVV20ZWv04HSHq8pN3zx5OhyLfm6z9M6iorspnZMEhRaRuVfkKRP9ttVuTStSAittmX0l/zxaGFrY21IB5p7Dv6325tPL7uHYcNr2Jm1tEgRzwlLQbOBurA+RGxtOSYY4CPkk1WuC8iXtDunP2EIr+mSqXNzEalNqDebd4B/RhwHLAOWCnpsoi4pXDM7sDHyYZo75RUukZ60VAj4SbXgrh/+ZLGvppKRkESuk9XnDI3WdM5c+Y19l37jm0+m8wsEQNsSo4A1kbEHQCSLiLLCnRL4ZhXAZdExJ0AEdFxid7R3f4zM5th9VpU2oqztfLt1KZTzQfuKjxfl+8regqwh6SrJV0n6S861W+4a0HU60O93FCUrVlsZkmoOgYcEcuAZe1OVVas6fkc4HCyVSLnAj+U9KOI+Gmrk1buAefBGDdIujx/7rT0Zpa0mqLSVsE6YGHh+QJgfckxV0TEbyPiPuB7wCFt69fFezkDWFN43n1a+okJmJhAhX+zXQqJ/cysnCpuFawEDpR0gKQdgZPJsgIVfR14vqQ5kuYBRzK9zdxG1VDkBcAfAucXdjstvZklbVA94IiYAE4HriRrVC+OiNWSlkhakh+zBrgCuBG4hmyqWtv4iKpjwB8F/gbYtbBvWlr6VlMu8sHsUwHmPPUkHA1nZsNSqw0uqCAilgPLm/Z9sun5vwL/WvWcVSLhXgrcExHXVT1pU4WmQpEXHum1IMxsaGoVt1Gp0gM+CnhZvuD6TsBjJX0Op6U3s8SNMsy4io6Nf0S8JyIWRMT+ZAPP/xkRf85UWnqomJa+cU73Gs1sCFJfjKefecBLcVp6M0tY6j3grhrgiLiabDnKntLSm5kNU+IJMZwV2czG16AW45kpQ22AKy9H2Xix8Fgl+5pf66ZMRWXLUQJs3rIZgOedM7UWx/VnHt77hcxs4FJvgPsJRe4+Lf2YmGx8zSxtqeeE66YHPBmK/NjCPqelN7Nkpd4DrtQAF0KR/xk4s9eLTa4H3BiKoENa97KXOn1a9VKm7enK1gOe29h33ZnOgmGWqsTvwVUegvgoWSjy1qb9HdPST8uKvO6aPqpqZtad1Icg+glFrpSWfloo8hOfA/X6eAViRGEzs6RUXZB9VHoORc6j4QCQdB5w+QzV0cysJ7XEe0Y9hyL3lJbe6wGb2RClPgTRzzzgD3Wblt7MbJjGORS5+7T0Y9hLLI5jj0OP3mycOBTZzGxE6uPUAzYzm03GYghC0s+BXwNbgImIWCRpT+BLwP5kY8CvjIhNVc7nP9vNbBhGme2iim7qd2xEHBoRi/Ln3WdFNjMbIikqbaPSzweEsyKbWdIGmJZ+RlRtgAO4StJ1eZZjaMqKDJRmRTYzG5VBpaWfKVVvwh0VEevz1PMrJN1a9QLT0tI/7Y+dlt7Mhib1WRCVesARsT7/eg9wKXAEeVZkgHZZkaetBeHG18yGKPVIuCqL8ewsadfJx8DxZGHHPWdFNjMbhtTHgKsMQewDXJqvdTAH+EJEXCFpJc6KbGYJm/ULskfEHcAhJfudFdnMkpZ6lIEj4cxsbKV+E84NsJmNrbEIRR6U0rT07RZM7iUtfafzdKlYv8nHjxSyIh919urG4xveuQgzS8dYhCJL+rmkm/L089fm+7bbtPRmNjukHorcTQ/42Ii4r2lfV2np9zzuHAA2ffuMagUGleF4QFmRJ80rZEV2r9csXan3gD0GbGZjK/Ux4H7WgoBu09Kvv7bvCpuZVVVXVNpGpWoDfFREHAacAJwm6Wh6SUu//1Gw42PYGlsb22wXEY3NzNIyyEg4SYsl3SZpraSWy+9KepakLZL+tNM5e14LIiI2RMSWiNgKnEe2PoSZWTIGtRqapDrwMbJO6MHAKZIObnHcB4ErK9WvwoVL14LoKS39lgnY4rT0ZjYcA1yM5whgbUTcERGbgYvI1kRv9lbgq7RYnKxZP2tBfNZp6c0sZbV+ggCmmw/cVXi+DjiyeICk+WSd0RcCz6py0n7Wgug6Lf3GK0/b9vxl36AhDKdWzUtXPG5rIxDjkca+F5w7tTTytW8/bJBVNLM+Vf3DtLhueW5ZRCwrHlJSrLml+ijwrojYUvUvYk9DM7OxVXWGQ97YLmtzyDpgYeH5AmB90zGLgIvyxvdxwImSJiLia61OOtQGeM8TPwXA/d+c+qCpqWQYegjDqVXHn4vHTdZ0zpx5jX0r3/7MQVbLzAZogE3JSuBASQcAdwMnA68qHhARBzSuK10IXN6u8YXqoci7S/qKpFslrZH0HEl7Sloh6fb8a+k8YDOzURnULIiImABOJ5vdsAa4OCJWS1oiaUmv9avaAz4buCIi/lTSjsA84G/J0tIvzefEvRt4V68VMTMbtEH+MR0Ry4HlTfs+2eLY11Y5Z5VpaI8FjgYuyE+8OSIeoJe09DVl2ziJwmZmSUl9MZ4qQxBPAu4F/l3SDZLOz+cDV0pLPy0U+a4fD6ziZmad1KVK26hUaYDnAIcBn4iIZwK/JRtuqGRaKPK+h8OEAzHMbDhST8pZpQFeB6yLiMnu61fIGuRKaenNzEal2EFqt41KxwY4In4J3CXpoHzXi4Bb6CUtfRdxf7NFFP6ZWVpS7wFXnQXxVuDz+QyIO4DXkTXeTktvZslKfZizUgMcEavIojyaOS29mSUr9T+4HYpsZmNrlDMcqnADbGZjK/UhiH5CkZ0V2cySNsD1gGdEP6HIL6HLrMhmZsOUeg+4YwNcCEV+LWShyMBmBx6YWepSb6b6CUWGClmRzcxGRRX/jUo/ociVsiJPWwti3TUDqbSZWRXjsBZEaShy1azI09aCWODEyWY2PKlHwvUcitxTVmQzsyFKfS2IfkKRz3FWZDNLWeL34PoKRe46K7KZ2TClPlvLkXBmNrZqifeB3QCb2dhKPQOaG2AzG1upR8JVScp5UGG9h1WSfiXp7U5Lb2apS30tiCrT0G6LiEMj4lDgcOAh4FKyYIzvRMSBwHfoIk+cmdkwjEMkXNGLgJ9FxP/QS1p6M7MhmvU94CYnA1/MH3eflt6hyGY2RDWp0jYqlW/C5UEYLwPe080FImIZsAzg4YkHssyVhffbNpll8SWV7Gt+rcp5ulSs3+TjR7Zsbux77kdXNx6v+quyrE1mNiqz/iZcwQnA9RGxIX/utPRmlrRaxW1UupmGdgpTww8wlZZ+KRXT0u954icA2Lj8zY19bSNVyl7q5QOtjw/Bsk/QeXPmNh7f8FeH935yM5tRqUfCVU1JNA84DriksHspcJyk2/PXlg6+emZm/Uh7PbSqa0E8BOzVtO9+uk1LX69nZQvjqqmP0XRUNk5tZklI/VfSkXBmNrakUY7wdjbcBnhiAhiDXm9B6mNMZtuz1H87qyTlPAj4UmHXk4C/B3YH3kSWLw7gbyNi+aAraGbWq9Q7ex0b4Ii4jSzvG5LqwN1kocivw2npzSxlif+F2u0QRCMUuZc/vTd+6y3Zg0EHYsygYv22NgIxHmnse/45axqPrz/TU9LMUpJ289tfKDJUSEtfDEW+4LwLe62nmVnXRK3SVulc0mJJt0laK2mbxcckvTpvD2+U9ANJh3Q8Z0S1LmUeirweeHpEbJC0D3AfWZ/0/cC+EfH6dueYe/y/BMD9y5c09tUSv0vZyVhNqTNLyE713fr+hbp503WVGrhn7HF422vlw68/JYt5WAesBE6JiFsKxzwXWBMRmySdALwvIo5sd96eQ5GrpqU3MxudgQViHAGsjYg7ImIzcBHZipANEfGDiNiUP/0RsKDTSbtpgKeFIveUlr5ebwRjjI0obGaWlKrrAReHSvPt1KZTzQfuKjxfl+9r5Q3AtzrVr9JNuEIocjH1/Ieclt7MUlZ1DKO4amMXpyrtdkk6lqwBfl6n6/YTitx9WnoHYpjZMA3u93MdsLDwfAHZPbGmy+l/AecDJ+TLNbTlUGQzG1u1wS02uRI4UNIBZLEQJwOvKh4gaT+yBcteExE/rXJSN8BmZh1ExISk04ErgTrw6YhYLWlJ/vonySKE9wI+nv9lPBERbbM0VB0DfgfwRrIxj5vIouDmkYUo7082BvzKwh3AVieqcrlZxdPQzNI1yCHCfKmF5U37Pll4/EaydrKyKmnp5wNvAxZFxDPIWv+TcVZkM0te2usBVx0gmQPMlTSHrOe7nj6yIkfhn5nZTEm7+a3QAEfE3cCHgTuBXwAPRsRVVMyKbGY2KlKt0jYqVYYg9iDr7R4A/B6ws6Q/r3oBp6U3s1GZ9T1g4MXAf0fEvRHxKNk0i+dSMStyRCyLiEURsWjOAkcrm9nwVI2EG5UqDfCdwLMlzVN2S/FFwBqmsiJDxazIZmbDlXYfuMqC7D+W9BXgemACuIEsZG8X4GJJbyBrpF8xkxU1M+tW6jNfq4Yivxd4b9PuR+g2K7KZ2VCl3QI7Es7MxtYAQ5FnhBtgMxtfaXeAh9sAb1z+5uzBoHPCdfom9xHvUZYTbvOWzY19R529uvH4hne2Dfs2syFLfXmASv1zSe+QtFrSzZK+KGknSe+TdLekVfl24kxX1sysG6lPQ+uYEy5fC+L7wMER8bCki8kWpNgf+E03aeknc8I1esLM/vV0vRiP2cwYRE64O3/7s0p//+6385NH8svbz1oQZmZJS70H3M9aENBlWnqHIpvZMFVLSp9wA9xmLYhPAE8GDiVrmD9SVn5aKPITnwP1+nithuaknGbpkqptI9LzWhBOS29mqZv1QxC0WAuip7T0ExMwMZHEGx8USY3NzNKS9koQ/a0Fcb7T0ptZ0hLvGHWchjZID088kF2s6vekaiBGK72U2aYK2wZiPLLlkca+55+zpvH4+jMP7/1CZjbNIKahbXj4rkq//fvMXTiSltqhyGY2tlIf5hxqD3gyEOP+5Usa+2ojTAcyCA7EMJsZg+gB3/O7dZUauL13WpBuIIakM/Iw5NWS3p7v21PSCkm3519L5wGbmY3KrJ8FIekZwJvIppkdArxU0oE4Lb2ZJW7WN8DA04AfRcRDETEBfJds2ln3aenr9WwbJw7EMLMeVWmAbwaOlrSXpHnAicBCKqalnxaKfNePB1VvM7OOaqpV2kZWv04HRMQa4IPACuAK4Cdk84ErmRaKvO/hDsQws6FJPRCjUtMfERdExGERcTSwEbidimnpzcxGZgzWgkDS3vnX/YCXA1+kl7T0I36zM2GsFhYyGzOp34SrGojxVUl7AY8Cp0XEJklLcVp6M0tY6t29qmnpn1+y7356TEvv4AUzG4bU2xeHIpvZ2Er95rgbYDMbW6n3gPsJRXZWZDNLW+KzIDr2gJtCkTcDV0j6Zv7yWd1kRTYzG6a0+7/9hSKbmSVtkNPQJC2WdJuktZK2WftGmXPy12+UdFinc/YTigwVsiKbmY1KMVK13VbhPHXgY8AJwMHAKZIObjrsBODAfDuVLHFxW/2EIlfKiuy09GY2KgPsAR8BrI2IOyJiM3AR2YJkRScB/xGZHwG7N+XO3FZEdLUBHwDe0rRvf+DmiuVP7eGaM14m1Xr5/fu9pFivYb2XYW1kPdZrC9upTa//KXB+4flrgHObjrkceF7h+XeARe2u23Mock9ZkTOnVjxu2GVSrdewyqRar17KpFqvXsqkWq9eyvRyjaGIwqJh+bas6ZCybnLz+gNVjpmmn1DkzzorspltJ9Yxde8LYAGwvodjpuknFPk1VcqamY2BlcCBkg4A7gZOBl7VdMxlZBMTLgKOBB6MfM30VkYRCdfctU+lTKr1GlaZVOvVS5lU69VLmVTr1UuZXq6RhIiYkHQ6cCVQBz4dEaslLclf/ySwnGyW2FrgIeB1nc471KzIZmY2ZXbnhDczm8XcAJuZjYgbYDOzEZnxm3CSnkoWITKfbMraeuCyyCLsqpR/HlkUys0RcdWMVdTMbMhmtAcs6V1kIXsCriGbyiGyQI5tFrPIy1xTePwm4FxgV+C9rcoMg6TdJC2VdKuk+/NtTb5v9xZlFjeVvyBfO+MLkvYZxTV6uU4v1xjWdcbpezZOP2NWzUwPQbwBeFZELI2Iz+XbUrIe7RtalNmh8PhU4LiI+AfgeODVZQWG9EN1MbAJOCYi9oqIvYBj831fbvFePlB4/BGyNTP+iOyD6FMjukYv1+nlGsO6zjh9z8bpZ2xydbAjJb1c0p/kj1NfIXK4Zji++lbgiSX7nwjc1qLMT4A9gL2Aa5teu6FFmSuBdwFPKOx7Qr5vRYsy1xcenw/8U16vdwBfKzm+tL7tXmu6xqqm11aN4hq9XKeXawzrOuP0PRuzn7HjyebDfiv//TqfbDGvtcDxreqwvW0zPQb8duA7km4H7sr37Qf8PnB6izK7AdeRDVWEpCdExC8l7ULr9ZX3j4gPFndExC+BD0p6fYV6LoqIQ/PHZ0n6y5Jj/kfS3wCfiYgNAHlP+bWF99Zsb0ln5vV+rCRF/tNJ+V8fw7hGL9fp5RrDus44fc/G6WfsbODFEfHz4k5lkWTLydYZ3+7N6BBERFwBPAX4B7Je6lXA+4CD8tfKyuwfEU+KiAPyr7/MX9pK64Xg/0fS3xSHDiTto2wMuu0PlaR3kv9QFV4r+778GVmv/LuSNknaCFwN7Am8ssU1ziMbv94F+AzwuLxuTwBWjegaZdfZlF9nrxbX6eUavbyfQbyXYX3PZut7mbzG1ZI29nCNCyu+lzlkayM0u5vpw4zbtbGIhFO2GPy7yWZb7J3v3kAWm700IjaVlHlv066PR8S9+Q/VhyLiL0rKPJVsgY0fRcRvCvsXt/pAycvMB35cpYykI4CIiJWSng4sBtZExPI2779Y5uC8zK3typSc47PRxfoekv6j7HvUoczzycb/b4oKM1pUYQaMpCPJ3uuDyhIGvBs4DFgNfCAiHqxY5pnALW3KvA24NCJafaD3dXxeZkfgFODuiPi2pFcDz83rtSwiHi0p8xiydQkmy7wqL7OmTZnfJ+vMLCRb2/unwBfL3ndJmQV5mdvblZH0HrIG/SKmOkEL87peHBH/0un7sT0Yiwa4HUmvi4h/77dM/gt1GtkP9qHAGRHx9fy16yNim/Qjkt5KNtRSqUz+oXACWe9hBVnj813gxcCVEfHPJddoLnMkWY+mXZnLSt72C4H/BIiIl/VzfKHcNRFxRP74jWTfv6+RjQ9+I7Ibsu2OPx24tNXx+XGrgUMii9VfBvwW+Crwonz/yyuUeQj4SocyD+bn/hnwBeDLEXFf2fsuOf6L+fH3tjo+L/N5sv/HucCDwM75+38R2e/qNkNjhTLzgAfIeqmX5GWIiNc2Hf824KXA98jWLVhFdgPuT8jW+b665Bpdl8nLHQy8jKwDIrIe8WURcUu778N2ZZgDzqPYgDsHUQa4Cdglf7w/2aLNZ+TPb2hxnq7K5MfXyX6ZfgU8Nt8/F7ixzTW6LXM98DngGOAF+ddf5I9fUHL8Dd0cXyxXeLwSeHz+eGeyXnBfx+evrSm+r6bXVg2wzA1kQ1PHAxcA95LdVPpLYNd+j8/L3Jh/nUP2F1w9f642/5ddlZn8eckfzwOuzh/v1+HnuKsy3qpto1gNbeAk3djqJaDVnMtuy9QjH0KIiJ9LOgb4iqQn0vrmYLdlJiJiC/CQpJ9FxK/ysg9L2triGr2UWQScAfwd8NcRsUrSwxHx3RbHH97l8ZNq+fBQjawHd29et99KmhjA8QA3F/5i+YmkRRFxraSnkK1fPagyERFbye5jXCVpB7K/PE4BPgw8vs/jJ9//jmQfOPPIbkhvBB5D63HTXsrMAbbkx+yaV/bOvI6tdFVG0m7Ae4A/LrzXe4Cvkw0LPtDmWtuNsWiAyRrMl5D9WVQk4AcDKvNLSYdGxCqAiPiNpJcCnwb+oMU1ui2zWdK8iHiIrNHLKpT9MLdqTLsukzcMZ0n6cv51A21+Fro9vqDbGS29zIB5I3C2pP8D3Af8UNJdZOOObxxgmWnXj2xs9TLgMklzB3A8ZD3lW8n+ovk74MuS7gCeTTaWOogy5wMrJf0IOJos3yOSHk/WcJfppczFZENUx0R+Iz2/v/JasvnGx7Uot30ZdRd8EBvZD+HzWrz2hUGUIbv58IQWxx/VYn9XZYDHtDj2ccAftHit6zIlx/4h2c2nqt/vro4vKT8POGCQx5P1yg4h+xDap+J5K5cBntLle+zq+EK53wN+L3+8O1kusiMGWQZ4en7MU7uoV1dl6GG+8fa4jf1NODMbPklXAd+mfL7xcRHx4hFWLxleDc3MZkJxTnPzfONXjLJiKXEP2MyGqpepoePKDbCZDZWkOyNiv1HXIwXjMgvCzBLSy9TQ7ZEbYDObCb1MDd3uuAE2s5lwOVkU6KrmFyRdPfTaJMpjwGZmI+JpaGZmI+IG2MxsRNwAm5mNiBtgM7MR+f9NhfKhF7ijxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_attentions shape : torch.Size([91, 96, 105])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "prediciton : thet at it there tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore tored tore \n",
      "Target :  uh... never mind.\n",
      "Distance Score: 584.385 \n",
      " Loss in Diarization: 2.465077571731677 Correct prediction: 82 se1_len: 278\n",
      "Teacher Forcing : 0.15\n",
      "epoch: 1\n",
      "0.001\n",
      "B: 1 / 9, avg_loss: 0.681, avg_loss_dia: 306.170, Time Taken : 2.219, \n",
      " "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_attentions shape : torch.Size([91, 103, 75])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       device='cuda:0')\n",
      "prediciton : that in  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou ther tour ther  ou\n",
      "Target :  uh... never mind.\n",
      "Distance Score: 582.691 \n",
      " Loss in Diarization: 2.448039802715933 Correct prediction: 82 se1_len: 278\n",
      "Teacher Forcing : 0.15\n",
      "epoch: 2\n",
      "0.001\n",
      "B: 1 / 9, avg_loss: 0.651, avg_loss_dia: 314.265, Time Taken : 2.345, \n",
      " "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-0bfb071c9c45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_dia\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdia_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-4d1c2fed8122>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epoch, teacherForcingRate, criterion_dia)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_dia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_dis = 1000\n",
    "for epoch in range(10):#New architecture with Encoder hidden dim=256 and Decoder hidden_dim=512 increased and encoder output increased\n",
    "    \n",
    "    tf=0.15\n",
    "        \n",
    "    print(\"Teacher Forcing :\",tf)\n",
    "    print(\"epoch:\", epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(param_group['lr'])\n",
    "    \n",
    "    train(model, train_loader, criterion, optimizer, epoch,tf, criterion_dia)\n",
    "    dis, correct, dia_loss, seq_len = validation(model, valid_loader)\n",
    "\n",
    "#     scheduler.step(dis)\n",
    "#    torch.save({'model_state': model.state_dict(), 'opti_state': optimizer.state_dict(), 'val_dis': dis}, \"epoch\"+str(epoch))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}